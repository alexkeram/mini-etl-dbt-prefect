    "# Teaching with a teacher - model quality\n",
    "## üìå Project goal\n",
    "To develop a model that predicts the probability of reducing the customer‚Äôs purchasing activity, and highlight segments for personalized marketing offers.  Link to the Hithab repository: https://github.com/alexkeram/teach-model\n",
    "## üîπ Step 1. Data loading\n",
    "- Download all the files provided:\n",
    "  - `/Datasets/money.csv`\n",
    "- Make sure that the data correspond to the description.\n",
    "- Check the separators and correctly consider the tables (special attention on `Sep = ';'` `and` decimal = ',' `).\n",
    "## üîπ Step 2. Data value\n",
    "- Check data types, passes, duplicates.\n",
    "- Transform numerical signs into the correct format (for example, with `Object` ‚Üí` float`).\n",
    "- Delete or fill out missing values.\n",
    "- Conduct primary cleaning and, if necessary, transcode categorical values.\n",
    "## üîπ Step 3. Research Analysis of Data (EDA)\n",
    "- Conduct visualization and descriptive analysis of the signs:\n",
    "  - Distributions,\n",
    "  - emissions,\n",
    "  - unique values ‚Äã‚Äãof categorical features.\n",
    "- identify customers having at least one purchase for the last 3 months.\n",
    "- Check the distribution of the target feature (`purchasing activity`).\n",
    "- identify potential correlations and anomalies.\n",
    "## üîπ Step 4.\n",
    "- combine the tables by `id`, transforming:\n",
    "  - `market_money.csv` in width for the periods:` revenue_ket`, `revenue_fire`,` revenue_d_ now.\n",
    "  - `market_time.csv` in a similar width:` Time_ket`, `Time_field`,` Time_D_ This.\n",
    "- attach to `market_file.csv`.\n",
    "- Save the table `money.csv` with a profit separately (use later for segmentation).\n",
    "## üîπ Step 5. Correlation analysis\n",
    "- Calculate the correlation matrix between numerical signs.\n",
    "- visualize the heat card.\n",
    "- if necessary - remove one of the strongly correlating signs (multicollinearity).\n",
    "## üîπ Step 6. Building of models using soldiers\n",
    "### 6.1 Preparation of Paydain\n",
    "- Use `Columntransformer` for separate processing of signs:\n",
    "  - Categorical: `ONEHOTENCODER`,` ORDINALENCODER`\n",
    "  - Numerical: `StandardScaler`,` minmaxScaler`\n",
    "- Gather Piplaine based on `pipeline`.\n",
    "### 6.2 Model training\n",
    "Teach the following models with the selection of hyperparameters:\n",
    "- `logisticregression`\n",
    "- `decisiontreeclassifier`\n",
    "- `Kneighborsclassifier`\n",
    "- `svc`\n",
    "### 6.3 Selection of metric\n",
    "- The main metric: ** roc auc ** (due to the task of binary classification and possible imbalance of classes).\n",
    "- Use `RandomizedSearchcv` or` Gridsearchcv` with cross-native.\n",
    "## üîπ Step 7. Model interpretation\n",
    "- Determine the best model by ROC AUC.\n",
    "- calculate the importance of signs for the best model.\n",
    "- Use the `Shap 'library to build a graph of importance.\n",
    "- Conclusion:\n",
    "  - what signs have had the greatest influence;\n",
    "  - which can be excluded;\n",
    "  - Business interpretation of the result.\n",
    "## üîπ Step 8. Clients segmentationand recommendations\n",
    "### 8.1 segmentation\n",
    "- combine these models (probabilities of reducing activity) and `money.csv`.\n",
    "- Create segments (approximately 2-3 segments) by:\n",
    "  - profit,\n",
    "  - share of promotional purchases,\n",
    "  - Categories of goods,\n",
    "  - probabilities of reducing activity.\n",
    "### 8.2 segment analysis\n",
    "- visualize the distributions and signs of the selected segment.\n",
    "- Describe business characteristics.\n",
    "- Make proposals to keep activity:\n",
    "  - Changing the conditions of mailings,\n",
    "  - New promotions,\n",
    "  - personalized proposals.\n",
    "### 8.3 Cods on segments\n",
    "- What segment is chosen.\n",
    "- What measures are proposed.\n",
    "- Why is it important for business.\n",
    "## üîπ Step 9. General conclusion\n",
    "- Briefly describe:\n",
    "  - The task,\n",
    "  - initial data and sources,\n",
    "  - Stages of expense,\n",
    "  - Model training and results,\n",
    "  - the selected model and metric,\n",
    "  - interpretation of signs,\n",
    "  - segmentation and business recommendations."
    "# General libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math \n",
    "from pathlib import Path \n",
    "\n",
    "# Visualization\n",
    "import matplotlib .pyplot as plt \n",
    "\n",
    "# Facing\n",
    "from sklearn .model_selection import train_test_split \n",
    "from sklearn .preprocessing import StandardScaler ,MinMaxScaler \n",
    "from sklearn .preprocessing import OneHotEncoder \n",
    "from sklearn .impute import SimpleImputer \n",
    "from sklearn .compose import ColumnTransformer \n",
    "from sklearn .pipeline import Pipeline \n",
    "\n",
    "# Models\n",
    "from sklearn .linear_model import LogisticRegression \n",
    "from sklearn .tree import DecisionTreeClassifier \n",
    "from sklearn .neighbors import KNeighborsClassifier \n",
    "from sklearn .svm import SVC \n",
    "\n",
    "# Selection of hyperparameters\n",
    "from sklearn .model_selection import RandomizedSearchCV \n",
    "\n",
    "# Metrics\n",
    "from sklearn .metrics import (\n",
    "roc_auc_score ,\n",
    "# Interpretation of signs\n",
    "import shap \n",
    "# Ignore warnings\n",
    "import warnings \n",
    "warnings .filterwarnings ('ignore')\n",
    "# Settings for visualization\n",
    "plt .rcParams ['figure.figsize']=(10 ,6 )\n"
    "# Step 1. Data loading\n",
    "\n",
    "# Ways to files\n",
    "\n",
    "# First we try the old option\n",
    "if Path ('/datasets/market_file.csv').exists ():\n",
    "    file_path_main ='/datasets/market_file.csv'\n",
    "    file_path_money ='/datasets/market_money.csv'\n",
    "    file_path_time ='/datasets/market_time.csv'\n",
    "    file_path_profit ='/datasets/money.csv'\n",
    "else :\n",
    "# Spare option\n",
    "    file_path_main ='data/market_file.csv'\n",
    "    file_path_money ='data/market_money.csv'\n",
    "    file_path_time ='data/market_time.csv'\n",
    "    file_path_profit ='data/money.csv'\n",
    "\n",
    "    # We load the tables\n",
    "df_main =pd .read_csv (file_path_main ,sep =',',decimal ='.')\n",
    "df_revenue =pd .read_csv (file_path_money ,sep =',',decimal ='.')\n",
    "df_time =pd .read_csv (file_path_time ,sep =',',decimal ='.')\n",
    "df_profit =pd .read_csv (file_path_profit ,sep =';',decimal =',')\n",
    "\n",
    "# View the first lines and information about tables\n",
    "print (\"market_file.csv:\")\n",
    "display (df_main .head ())\n",
    "print (df_main .info (),'\\n')\n",
    "\n",
    "print (\"market_money.csv:\")\n",
    "display (df_revenue .head ())\n",
    "print (df_revenue .info (),'\\n')\n",
    "\n",
    "print (\"market_time.csv:\")\n",
    "display (df_time .head ())\n",
    "print (df_time .info (),'\\n')\n",
    "\n",
    "print (\"money.csv:\")\n",
    "display (df_profit .head ())\n",
    "print (df_profit .info ())\n"
    "### conclusion by step 1: Data loading\n",
    "The data are successfully loaded and generally correspond to the description. All tables have the correct structure and the required number of lines. The detected minor inconsistencies in individual values ‚Äã‚Äã(for example, typos in the names of categories or periods) will be processed in the corresponding sections of the exponent and data purification sections. You can proceed to the next stage."
    "# Data reporting"
    "# Step 2.\n",
    "\n",
    "# Checking data types, passes and duplicates in all tables\n",
    "\n",
    "def check_df (df ,name ):\n",
    "    print (f'\\n===== {name } =====')\n",
    "    print ('Form:',df .shape )\n",
    "    print ('Duplicate:',df .duplicated ().sum ())\n",
    "    print ('Data types:')\n",
    "    print (df .dtypes )\n",
    "    print ('\\ nPPRUSIONS ON THE PLASS:')\n",
    "    print (df .isna ().sum ())\n",
    "    print ('-'*50 )\n",
    "\n",
    "check_df (df_main ,'Main table (Market_file)')\n",
    "check_df (df_revenue ,'Revenue (Market_Money)')\n",
    "check_df (df_time ,'Time on the site (Market_time)')\n",
    "check_df (df_profit ,'Profit (Money)')\n"
    "# Visualization of passes in the main table\n",
    "plt .figure (figsize =(10 ,4 ))\n",
    "plt .bar (df_main .columns ,df_main .isna ().sum ())\n",
    "plt .xticks (rotation =90 )\n",
    "plt .title ('Passes in the main table')\n",
    "plt .ylabel ('The number of passes')\n",
    "plt .grid (axis ='y')\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n"
    "# Transformation of types\n",
    "# Check if there are non -layer data in numerical columns (just in case)\n",
    "for col in df_main .select_dtypes (include ='object').columns :\n",
    "    print (f'{col }: {df_main [col ].unique ()[:5 ]}...')# The first 5 unique values\n"
    "# Remove duplicates if there is\n",
    "df_main =df_main .drop_duplicates ()\n",
    "df_revenue =df_revenue .drop_duplicates ()\n",
    "df_time =df_time .drop_duplicates ()\n",
    "df_profit =df_profit .drop_duplicates ()\n"
    "# Correct the typo in the \"Type of Service\" (standard ‚Üí standard)\n",
    "df_main ['Type of service']=df_main ['Type of service'].replace ('standard','standard')\n",
    "# Correct the typo in the \"period\" in Market_time\n",
    "df_time ['Period']=df_time ['Period'].replace ({'Previous_xyats':'Previous_xiac'})\n"
    "# Let us bring the values ‚Äã‚Äãin \"allow\" to \"to\" to the binary form (yes ‚Üí 1, no ‚Üí 0)\n",
    "df_main ['Allow to report']=df_main ['Allow to report'].map ({'Yes':1 ,'No':0 })\n"
    "# Size check after cleaning\n",
    "print (f'Main table: {df_main .shape}')\n",
    "print (f'Revenue: {df_revenue .shape}')\n",
    "print (f'Time on the site: {df_time .Shape}')\n",
    "print (f'Profit: {df_profit .shape}')\n"
    "# Let's look at the unique values ‚Äã‚Äãof categorical variables\n",
    "cat_columns =['Buying activity','Type of service','Popular_Acategoria']\n",
    "for col in cat_columns :\n",
    "    print (f'\\n{col }:')\n",
    "    print (df_main [col ].value_counts ())\n"
    "# We build histograms for all numerical signs\n",
    "# Numerical signs\n",
    "num_columns =df_main .select_dtypes (include =['int64','float64']).columns \n",
    "# The parameters of the grid\n",
    "cols =3 \n",
    "rows =math .ceil (len (num_columns )/cols )\n",
    "plt .figure (figsize =(5 *cols ,4 *rows ))\n",
    "for i ,col in enumerate (num_columns ):\n",
    "    plt .subplot (rows ,cols ,i +1 )\n",
    "    plt .hist (df_main [col ],bins =20 ,edgecolor ='black')\n",
    "    plt .title (col )\n",
    "    plt .tight_layout ()\n",
    "plt .suptitle ('Distribution of numerical features',y =1.02 ,fontsize =16 )\n",
    "plt .show ()\n",
    "# Distribution graphs of categorical features\n",
    "plt .figure (figsize =(15 ,6 ))\n",
    "for i ,col in enumerate (cat_columns ):\n",
    "    plt .subplot (1 ,3 ,i +1 )\n",
    "    df_main [col ].value_counts ().plot (kind ='bar')\n",
    "    plt .title (col )\n",
    "    plt .xticks (rotation =45 )\n",
    "    plt .tight_layout ()\n",
    "plt .suptitle ('Distribution of categorical features',y =1.05 ,fontsize =16 )\n",
    "plt .show ()\n"
    "# Check 1: The main Dataset (should be 1 line for the client)\n",
    "duplicate_main =df_main [df_main .duplicated (subset ='id',keep =False )]\n",
    "print (f'Repeating ID in Market_file: {duplicate_main .shape [0]}')\n",
    "if not duplicate_main .empty :\n",
    "    display (duplicate_main )\n",
    "\n",
    "    # Check 2: Revenue - implicit duplicates according to ID + period\n",
    "duplicate_revenue =df_revenue [df_revenue .duplicated (subset =['id','Period'],keep =False )]\n",
    "print (f'Implicit duplicates in Market_Money software (ID, period): {duplicate_revenue .shape [0]}')\n",
    "if not duplicate_revenue .empty :\n",
    "    display (duplicate_revenue .sort_values (by =['id','Period']))\n",
    "\n",
    "    # Check 3: Time - implicit duplicates according to ID + period\n",
    "duplicate_time =df_time [df_time .duplicated (subset =['id','Period'],keep =False )]\n",
    "print (f'Implicit duplicates in market_time software (id, period): {duplicate_time .shape [0]}')\n",
    "if not duplicate_time .empty :\n",
    "    display (duplicate_time .sort_values (by =['id','Period']))\n",
    "\n",
    "    # Check 4: Profit table - there should be one ID per line\n",
    "duplicate_profit =df_profit [df_profit .duplicated (subset ='id',keep =False )]\n",
    "print (f'Repeating ID in money.csv: {duplicate_profit .shape [0]}')\n",
    "if not duplicate_profit .empty :\n",
    "    display (duplicate_profit )\n"
    "## Analysis and removal of revenue emissions"
    "Purpose: to exclude customers with abnormally high revenue so that the model is not distorted by extreme values."
    "# Boxed to the revenue\n",
    "plt .figure (figsize =(10 ,1 ))\n",
    "plt .boxplot (df_revenue ['Revenue'],vert =False )\n",
    "plt .title ('BOXPLOT RECOUNESS (Market_Money)')\n",
    "plt .xlabel ('Revenue')\n",
    "plt .grid (True ,axis ='x')\n",
    "plt .show ()"
    "We cut off the radical emission on top, without touching the smaller values"
    "# Calculation of borders according to the method of inter -apartment scope\n",
    "Q1 =df_revenue ['Revenue'].quantile (0.25 )\n",
    "Q3 =df_revenue ['Revenue'].quantile (0.75 )\n",
    "IQR =Q3 -Q1 \n",
    "upper_bound =Q3 +1.5 *IQR \n",
    "print (f'Restriction from above: {upper_bound: .2f}')\n",
    "# Filter: leave only reasonable revenue values\n",
    "df_revenue =df_revenue [df_revenue ['Revenue']<=upper_bound ]"
    "# Boxed to the revenue\n",
    "plt .figure (figsize =(10 ,1 ))\n",
    "plt .boxplot (df_revenue ['Revenue'],vert =False )\n",
    "plt .title ('BOXPLOT RECOUNESS (Market_Money)')\n",
    "plt .xlabel ('Revenue')\n",
    "plt .grid (True ,axis ='x')\n",
    "plt .show ()\n"
    "## filtering inactive customers"
    "Target:\n",
    "Remove customers from analysis who did not buy anything in at least one month\n",
    "- We consider the total revenue for each ID.\n",
    "- We leave only those who had purchases\n",
    "- Find all the tables for these IDs."
    "# We convert the table into a wide format\n",
    "revenue_raw =pd .read_csv ('data/market_money.csv')\n",
    "revenue_pivot =revenue_raw .pivot (index ='id',columns ='Period',values ='Revenue')\n",
    "# We will find customers who have at least one month = 0\n",
    "ids_to_remove =revenue_pivot [revenue_pivot .isna ().any (axis =1 )|(revenue_pivot ==0 ).any (axis =1 )].index \n",
    "print (f'We delete customers: {len (ids_to_remove)} - {list (ids_to_remove)}')\n",
    "# We filter all the tables\n",
    "df_main =df_main [~df_main ['id'].isin (ids_to_remove )]\n",
    "df_revenue =df_revenue [~df_revenue ['id'].isin (ids_to_remove )]\n",
    "df_time =df_time [~df_time ['id'].isin (ids_to_remove )]\n",
    "df_profit =df_profit [~df_profit ['id'].isin (ids_to_remove )]\n",
    "print (f\"There are customers left after filtration: {df_main ['id']. Nunique ()}\")"
    "### Conclusion by step 2: data reporting\n",
    "\n",
    "- ** Data types ** in all tables are given to the correct format, numerical and categorical features are recognized correctly.\n",
    "- ** Passes ** and**duplicates ** in the main table (`market_file.csv`) are absent.\n",
    "- were eliminated ** typos ** in the data: `` Standatt '‚Üí' Standard'`, `'' previous_ mixed '‚Üí' 'previous_xyats'``.\n",
    "- The meaning of the sign `Allocate to report` is given to the binary format:` yes` ‚Üí `1`,` no` ‚Üí `0`.\n",
    "- The target feature of `Consumeric activity` contains two classes with moderate imbalance and is ready for binary encoding.\n",
    "- Categorical features (`Service type,` Popular category`) have an adequate distribution and are prepared for subsequent coding.\n",
    "- With a clock, numerical signs do not contain obvious emissions, except for one radical emission in the table with the revenue that was filtered, the majority are distributed normally or discretely.\n",
    "- In the tables `market_money` and` market_time` did not reveal duplicate duplicates according to the composition of the composition of `id + period`.\n",
    "- In the `money.csv` table, each` id` is once found, the structure is correct."
    "# Research Data Analysis (EDA)"
    "# We build the distribution of the target attribute\n",
    "plt .figure (figsize =(6 ,4 ))\n",
    "df_main ['Buying activity'].value_counts ().plot (kind ='bar')\n",
    "plt .title ('Distribution of purchasing activity among active customers')\n",
    "plt .ylabel ('The number of customers')\n",
    "plt .xticks (rotation =0 )\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n",
    "\n",
    "# In percentage\n",
    "print (round (df_main ['Buying activity'].value_counts (normalize =True )*100 ,2 ))\n"
    "# Leave only numerical features\n",
    "numeric_features =df_main .drop (columns =['id','Buying activity']).select_dtypes (include =['int64','float64'])\n",
    "\n",
    "# We calculate the correlation matrix\n",
    "corr =numeric_features .corr (method ='spearman')\n",
    "\n",
    "# Building a heat card\n",
    "plt .figure (figsize =(12 ,10 ))\n",
    "im =plt .imshow (corr ,cmap ='RdYlGn',vmin =-1 ,vmax =1 )\n",
    "\n",
    "# Signatures of the axes\n",
    "plt .xticks (ticks =np .arange (len (corr .columns )),labels =corr .columns ,rotation =90 )\n",
    "plt .yticks (ticks =np .arange (len (corr .index )),labels =corr .index )\n",
    "\n",
    "# Adding values ‚Äã‚Äãto a thermal card\n",
    "for i in range (len (corr .index )):\n",
    "    for j in range (len (corr .columns )):\n",
    "        plt .text (j ,i ,f\"{corr .iloc [i ,j ]:.2f}\",\n",
    "        ha =\"center\",va =\"center\",color =\"black\",fontsize =8 )\n",
    "\n",
    "        # Color\n",
    "plt .colorbar (im )\n",
    "plt .title ('Correlation matrix of numerical signs')\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n"
    "# List of numerical features, excluding ID\n",
    "num_cols =df_main .select_dtypes (include =['int64','float64']).drop (columns ='id').columns \n",
    "\n",
    "# We create a summary summary\n",
    "outliers_summary ={}\n",
    "\n",
    "for col in num_cols :\n",
    "    q1 =df_main [col ].quantile (0.25 )\n",
    "    q3 =df_main [col ].quantile (0.75 )\n",
    "    iqr =q3 -q1 \n",
    "    lower =q1 -1.5 *iqr \n",
    "    upper =q3 +1.5 *iqr \n",
    "    outliers =df_main [(df_main [col ]<lower )|(df_main [col ]>upper )]\n",
    "    outliers_summary [col ]=len (outliers )\n",
    "\n",
    "    # The conclusion of the number of emissions for each basis\n",
    "for col ,count in outliers_summary .items ():\n",
    "    print (f'{color}: {Count} emissions')\n"
    "# Visualization of emissions\n",
    "cols =3 \n",
    "rows =(len (num_cols )+cols -1 )//cols \n",
    "plt .figure (figsize =(5 *cols ,4 *rows ))\n",
    "for i ,col in enumerate (num_cols ):\n",
    "    plt .subplot (rows ,cols ,i +1 )\n",
    "    plt .boxplot (df_main [col ],vert =False )\n",
    "    plt .title (col )\n",
    "plt .suptitle ('BoxPlot for numerical signs',fontsize =16 ,y =1.02 )\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n"
    "# Counting the number of customers by categories\n",
    "activity_counts =df_main ['Buying activity'].value_counts ()\n",
    "activity_labels =activity_counts .index \n",
    "activity_values =activity_counts .values \n",
    "\n",
    "# Building a graph\n",
    "plt .figure (figsize =(6 ,4 ))\n",
    "bars =plt .bar (activity_labels ,activity_values ,color =['#4CAF50','#F44336'])\n",
    "\n",
    "# Add annotations over the columns\n",
    "for bar in bars :\n",
    "    height =bar .get_height ()\n",
    "    plt .text (bar .get_x ()+bar .get_width ()/2 ,height +10 ,f'{height }',ha ='center',va ='bottom')\n",
    "\n",
    "plt .title ('The number of clients by activity')\n",
    "plt .ylabel ('The number of customers')\n",
    "plt .xticks (rotation =0 )\n",
    "plt .grid (axis ='y',linestyle ='--',alpha =0.5 )\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n",
    "print (f'Number of active customers: {df_main .shape [0]}')"
    "### conclusion by step 3: Research analysis of data\n",
    "\n",
    "- Clients with purchases: 1297 had revenue in each month of the period.\n",
    "- Most customers (802) remained active, 495 - reduced activity.\n",
    "- The target feature: ~ 61.7% of customers retained the previous level of activity, ~ 38.3% - reduced. The classes are modeled moderately.\n",
    "- Correlations:\n",
    "    - There were no strong multi - - - -halinearity between the signs.\n",
    "    - The most pronounced positive correlations were revealed between signs of behavior on the site: `pages_za_vizit`,` average_ -recruiting_Kategoria_za_zit`, `Duration`.\n",
    "- emissions:\n",
    "    - the largest number of emissions according to the signs of `Market_activ_tex_,` Active_Carus`, `unpaid_products_Stuk_Cartal`.\n",
    "    - emissions are preserved as a potentially significant part of client behavior."
    "# Tables Association"
    "# Turn revenue\n",
    "df_revenue_wide =df_revenue .pivot (index ='id',columns ='Period',values ='Revenue').reset_index ()\n",
    "df_revenue_wide .columns .name =None # Remove the name of the column level\n",
    "df_revenue_wide =df_revenue_wide .rename (columns ={\n",
    "'Current_xiac':'Revenue_ -flowing',\n",
    "'Previous_xiac':'Revenue_ predecessor',\n",
    "'Permissive_xiac':'Revenue_ -reinforcing'\n",
    "# Time turn\n",
    "df_time_wide =df_time .pivot (index ='id',columns ='Period',values ='minutes').reset_index ()\n",
    "df_time_wide .columns .name =None \n",
    "df_time_wide =df_time_wide .rename (columns ={\n",
    "'Current_xiac':'Time_noving',\n",
    "'Previous_xiac':'Time_ predecessor',\n",
    "'Permissive_xiac':'Time_ -rehabilitable'\n",
    "# Combining tables\n",
    "df_full =df_main .merge (df_revenue_wide ,on ='id',how ='left')\n",
    "df_full =df_full .merge (df_time_wide ,on ='id',how ='left')\n",
    "# Check the size\n",
    "print (f'The size of the combined table: {df_full .Shape [0]')\n"
    "### histograms and boxplot‚Äôs on numerical characteristics, for categorical signs - column diagrams for groups"
    "# Divide the data into groups\n",
    "df_active =df_full [df_full ['Buying activity']=='Previous level']\n",
    "df_dropped =df_full [df_full ['Buying activity']=='Decreased']\n"
    "# Compare the numerical features\n",
    "# List of numerical signs without ID\n",
    "num_cols =df_full .select_dtypes (include =['int64','float64']).drop (columns ='id').columns \n",
    "\n",
    "# We build the cashed histograms\n",
    "cols =3 \n",
    "rows =(len (num_cols )+cols -1 )//cols \n",
    "plt .figure (figsize =(5 *cols ,4 *rows ))\n",
    "\n",
    "for i ,col in enumerate (num_cols ):\n",
    "    plt .subplot (rows ,cols ,i +1 )\n",
    "    plt .hist (df_active [col ],bins =20 ,alpha =0.6 ,label ='Previous level')\n",
    "    plt .hist (df_dropped [col ],bins =20 ,alpha =0.6 ,label ='Decreased')\n",
    "    plt .title (col )\n",
    "    plt .legend ()\n",
    "\n",
    "plt .tight_layout ()\n",
    "plt .suptitle ('Distribution of numerical features by target',y =1.02 ,fontsize =16 )\n",
    "plt .show ()\n"
    "# Categorical features\n",
    "col ='Popular_Acategoria'\n",
    "ctab =pd .crosstab (df_full [col ],df_full ['Buying activity'],normalize ='index')\n",
    "\n",
    "ctab .plot (kind ='barh',stacked =True ,figsize =(8 ,6 ))# Horizontal diagram\n",
    "plt .title (f'{color} by activity groups')\n",
    "plt .xlabel ('Share')\n",
    "plt .ylabel ('Category')\n",
    "plt .grid (axis ='x')\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n"
    "# Horizontal graph for 'Type Service'\n",
    "col ='Type of service'\n",
    "ctab =pd .crosstab (df_full [col ],df_full ['Buying activity'],normalize ='index')\n",
    "\n",
    "ctab .plot (kind ='barh',stacked =True ,figsize =(6 ,3 ))# Horizontal diagram\n",
    "plt .title (f'{color} by activity groups')\n",
    "plt .xlabel ('Share')\n",
    "plt .ylabel ('Type of service')\n",
    "plt .grid (axis ='x')\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n"
    "### conclusion by step 4: Table association\n",
    "- the `market_money.csv` table is converted from a long format into wide, with columns:` revenue_tek`, `revenue_field`,` revenue_do_ this.\n",
    "- the `market_time.csv` table is transformed similarly to the columns:` Time_te`, `Time_field`. A sign of `time_d_e is absent in the source data.\n",
    "- All transformed tables are combined with the main (`market_file.csv`) by the key` id`. The size of the final table: 1300 lines and 18 signs.\n",
    "- The profit table (`money.csv`) contains unique` id`, completely coinciding with the main table, and will be used later at the segmentation stage.\n",
    "Based on a visual analysis of distributions, one can distinguish the features of customer behavior whose purchasing activity has decreased:\n",
    "- ** communications: ** less often agree to mailings, receive less marketing touches.\n",
    "- ** User behavior: ** Viewing fewer pages and categories, spend less time on the site.\n",
    "- ** Promotions and purchases: ** Make fewer promotional purchases, less goods are left in the basket.\n",
    "- ** financial indicators: ** they have below revenue in each of the three months.\n",
    "- ** Categories of interests: ** more often buy cosmetics and dishes; Less commonly - goods for children and equipment.\n",
    "- ** Type of service: ** Among them, the tariff `standard` is more common.\n",
    "Thus, customers with reduced activity demonstrate signs of reducing involvement and less loyalty to the store. These behavioral features can be used to build a model and subsequent segmentation."
    "# Preparation of signs and target column\n",
    "# Target sign\n",
    "df_full ['Buying activity']=df_full ['Buying activity'].map ({\n",
    "'Decreased':1 ,\n",
    "'Previous level':0 \n",
    "# Target variable and signs\n",
    "target ='Buying activity'\n",
    "X =df_full .drop (columns =[target ])\n",
    "y =df_full [target ]\n",
    "# List of signs\n",
    "cat_features =['Type of service','Popular_Acategoria']\n",
    "num_features =[col for col in X .columns if col not in ['id']+cat_features ]\n",
    "# Separation into training and test samples\n",
    "X_train ,X_test ,y_train ,y_test =train_test_split (\n",
    "X ,y ,test_size =0.25 ,stratify =y ,random_state =42 \n",
    "# Categorical and numerical features\n",
    "cat_features =df_main .select_dtypes (include ='object').drop (columns ='Buying activity').columns .tolist ()\n",
    "num_features =df_main .select_dtypes (include =['int64','float64']).drop (columns ='id').columns .tolist ()\n",
    "\n",
    "# Categorical transformer\n",
    "cat_transformer =Pipeline (steps =[\n",
    "('imputer',SimpleImputer (strategy ='most_frequent')),\n",
    "('onehot',OneHotEncoder (drop ='first'))\n",
    "# Numerical transformers\n",
    "num_transformer_standard =Pipeline (steps =[\n",
    "('imputer',SimpleImputer (strategy ='median')),\n",
    "('scaler',StandardScaler ())\n",
    "num_transformer_minmax =Pipeline (steps =[\n",
    "('imputer',SimpleImputer (strategy ='median')),\n",
    "('scaler',MinMaxScaler ())\n",
    "# Two preprocessor options\n",
    "preprocessor_options ={\n",
    "'standard':ColumnTransformer ([\n",
    "('num',num_transformer_standard ,num_features ),\n",
    "('cat',cat_transformer ,cat_features )\n",
    "]),\n",
    "'minmax':ColumnTransformer ([\n",
    "('num',num_transformer_minmax ,num_features ),\n",
    "('cat',cat_transformer ,cat_features )\n",
    "])\n",
    "# Dictionary of models with hyperparameters, including a preprocessor\n",
    "models ={\n",
    "'LogisticRegression':{\n",
    "'model':LogisticRegression (max_iter =1000 ),\n",
    "'params':{\n",
    "'preprocessor':[preprocessor_options ['standard'],preprocessor_options ['minmax']],\n",
    "'model__C':[0.01 ,0.1 ,1 ,5 ,10 ]\n",
    "}\n",
    "},\n",
    "'DecisionTree':{\n",
    "'model':DecisionTreeClassifier (random_state =42 ),\n",
    "'params':{\n",
    "'preprocessor':[preprocessor_options ['standard'],preprocessor_options ['minmax']],\n",
    "'model__max_depth':[3 ,5 ,8 ,10 ,15 ],\n",
    "'model__min_samples_split':[2 ,4 ,5 ,10 ]\n",
    "}\n",
    "},\n",
    "'KNeighbors':{\n",
    "'model':KNeighborsClassifier (),\n",
    "'params':{\n",
    "'preprocessor':[preprocessor_options ['standard'],preprocessor_options ['minmax']],\n",
    "'model__n_neighbors':[3 ,5 ,7 ,9 ],\n",
    "'model__weights':['uniform','distance']\n",
    "}\n",
    "},\n",
    "'SVC':{\n",
    "'model':SVC (probability =True ),\n",
    "'params':{\n",
    "'preprocessor':[preprocessor_options ['standard'],preprocessor_options ['minmax']],\n",
    "'model__C':[0.1 ,1 ,10 ],\n",
    "'model__kernel':['linear','rbf']\n",
    "}\n",
    "}\n",
    "best_models ={}\n",
    "\n",
    "for name ,item in models .items ():\n",
    "    print (f\"\\ n ====== Search by model: {name} ========\")\n",
    "\n",
    "    # We collect a common pickle with a preprocessor player\n",
    "    pipe =Pipeline (steps =[\n",
    "    ('preprocessor','passthrough'),# Gridsearch will be replaced\n",
    "    ('model',item ['model'])\n",
    "    search =RandomizedSearchCV (\n",
    "    pipe ,\n",
    "    item ['params'],\n",
    "    scoring ='roc_auc',\n",
    "    cv =5 ,\n",
    "    n_iter =10 ,\n",
    "    random_state =42 ,\n",
    "    n_jobs =-1 \n",
    "\n",
    "    # Education\n",
    "    search .fit (X_train ,y_train )\n",
    "    y_pred_proba =search .predict_proba (X_test )[:,1 ]\n",
    "    score =roc_auc_score (y_test ,y_pred_proba )\n",
    "    print (f\"ROC AUC: {score :.4f}\")\n",
    "\n",
    "    # We keep the result\n",
    "    best_models [name ]={\n",
    "    'model':search .best_estimator_ ,\n",
    "    'score':score \n",
    "### conclusion in step 6: Building models\n",
    "\n",
    "- Four models were trained (`LogisticregResion`,` Decisiontree`, `Kneighbors`,` SVC`) using the fees and the selection of hyperparameters via `randomizedSearchcv`.\n",
    "- RoC AUC was used as the main metric, since the task is a binary classification with a moderate class imbalance.\n",
    "- For all models, two options for scaling signs were tested: `Standardscler` and` MinmaxScaler ', as required by the task.\n",
    "- The best performance was shown by the `SVC` model, reaching ROC AUC = ** 0.9019 **.\n",
    "- High results also showed:\n",
    "  - `LogisticregResion` - ROC AUC = ** 0.9012 **\n",
    "  - `kneighborsclassifier` - ROC AUC = ** 0.8876 **\n",
    "- Model `decisiontree` showed the smallest efficiency - ROC AUC = ** 0.8729 **.\n",
    "- For further analysis of the importance of the signs and interpretation of the model, ** model `svc` **, as showing the best quality of classification, will be used."
    "# Interpretation of the model"
    "# Find a model with the highest ROC AUC\n",
    "best_key =max (best_models ,key =lambda k :best_models [k ]['score'])\n",
    "best_model =best_models [best_key ]['model']\n",
    "print (f'Best model: {Best_key} with ROC AUC = {BEST_MODELS [BEST_KEY] [\"Score\"] :. 4F}')\n",
    "# We extract a preprocessor and a model from a feather\n",
    "preprocessor =best_model .named_steps ['preprocessor']\n",
    "model =best_model .named_steps ['model']\n",
    "print (preprocessor .transformers [0 ][1 ].named_steps ['scaler'])"
    "# We convert a training sample\n",
    "X_train_transformed =preprocessor .transform (X_train )\n",
    "# We get the names of categorical signs (Onehotencoder with Drop = 'FIRST')\n",
    "ohe =preprocessor .named_transformers_ ['cat'].named_steps ['onehot']\n",
    "ohe_feature_names =ohe .get_feature_names_out (cat_features )\n",
    "# We combine with numerical ones\n",
    "all_feature_names =list (ohe_feature_names )+num_features "
    "# Find the best LogisticregRESSION on Score\n",
    "log_keys =[k for k in best_models if k .startswith ('LogisticRegression')]\n",
    "best_log_key =max (log_keys ,key =lambda k :best_models [k ]['score'])\n",
    "log_model =best_models [best_log_key ]['model']\n",
    "\n",
    "# We get a preprocessor and trained data\n",
    "preprocessor =log_model .named_steps ['preprocessor']\n",
    "model =log_model .named_steps ['model']\n",
    "\n",
    "# We convert a training sample\n",
    "X_train_transformed =preprocessor .transform (X_train )\n",
    "\n",
    "# We get the names of the signs\n",
    "ohe =preprocessor .named_transformers_ ['cat'].named_steps ['onehot']\n",
    "ohe_feature_names =ohe .get_feature_names_out (cat_features )\n",
    "all_feature_names =num_features +list (ohe_feature_names )\n",
    "\n",
    "# We convert the array into Dataframe\n",
    "X_train_df =pd .DataFrame (X_train_transformed ,columns =all_feature_names )\n",
    "\n",
    "# Shap for Logisticregression\n",
    "explainer =shap .Explainer (model ,X_train_df )\n",
    "shap_values =explainer (X_train_df )\n",
    "\n",
    "# Schedule\n",
    "shap .summary_plot (\n",
    "shap_values .values ,\n",
    "features =X_train_df ,\n",
    "feature_names =all_feature_names ,\n",
    "show =True ,\n",
    "plot_size =(18 ,13 )\n",
    "### conclusion in step 7: Model interpretation\n",
    "- The following signs had the greatest impact on the prediction of a decrease in customer activity:\n",
    "  - ** `Pages_za_vizit` ** and **` average_ -seam_kategoria_za_zit` ** - users who are more actively studying the site have a lower risk of reducing activity.\n",
    "  - ** `unpaid_P products_STUK_Kartal` ** - a large number of abandoned goods in the basket are positively associated with the risk of outflow.\n",
    "  - ** `Active_ -purchases` ** - frequent participation in shares - an important factor that shifts the prediction towards the outflow.\n",
    "  - ** `Market_activ_6_Mes` ** - the long -term exposure to marketing campaigns is associated with increased risk, especially with high values.\n",
    "- A significant effect is also exerted:\n",
    "  - Some ** categorical signs **, especially buyers who prefer the categories of ‚Äúsmall household appliances‚Äù and ‚Äútechnology for beauty‚Äù - they are more likely to apply to customers with reduced loyalty.\n",
    "  - ** `Error_Service` ** and **` Duration` ** (in days from the moment of registration) - reflect the customer‚Äôs user experience and maturity.\n",
    "- The less significant for the model turned out to be:\n",
    "  - ** `Market_activ_Tec_xmes` **, **` Type of service_standart` **, ** `Allocate to report` **, as well as popular categories\" Cosmetics \",\" kitchen dishes \"and\" goods for children \".\n",
    "- ** Conclusion: ** The most informative for the model are the signs that reflect ** the involvement of the client on the site ** and ** his reaction to marketing activity **. These data should be taken into account when segmentation of customers and the formation of personalized deduction strategies."
    "# Customer segmentation and recommendations"
    "### Step 8: Customer segmentation and recommendations\n",
    "### the goal of segmentation\n",
    "In the previous stages, we studied the behavior of customers who have reduced activity and determined the key features affecting the outflow: low involvement on the site, orientation on stocks, a smaller volume of revenue.\n",
    "Now, on the basis of the model and probabilities of reducing activity, we will single out specific customer groups for further analysis and personalized actions. The task is to determine which segments are most important to retain or require attention from the point of view of marketing.\n",
    "#### Logic selection selection\n",
    "I decided to consider ** three segments **, each of which has business significance:\n",
    "- ** segment a: **\n",
    "  Clients with a high probability of reducing activity ** and at the same time ** high profit **.\n",
    "  This is a key group for priority retention - the loss of these customers will be the most painful for business.\n",
    "- ** segment b: **\n",
    "  Clients with ** high share of promotional purchases ** and ** average profit **.\n",
    "  The risk of forming dependence on discounts and, as a result, a decrease in marginality is possible. The segment is important for rethinking marketing strategies.\n",
    "- ** segment C: **\n",
    "  Customers buying ** goods for children **, and at the same time having ** increased risk of outflow **.\n",
    "  The possible sign that in the ‚ÄúProducts for Children‚Äù category there are problems with interest/retention, despite the importance of this audience (for example, young families).\n",
    "#### Why are the three segments chosen\n",
    "During segmentation, we do not just share users, but select groups with different business risks and behavioral patterns:\n",
    "- ** The A ** segment covers customers whose loss is the most critical due to high profit.\n",
    "- ** segment b ** allows you to evaluate the effectiveness of marketing and identify customers prone to discounts.\n",
    "- ** The C ** segment is important for analysis of behavior in a particular product category with potential risk.\n",
    "Such a choice covers three different business hypotheses: holding valuable customers, margin management, and adjusting commodity strategies."
    "# 1. Predictions of the probability of outflow\n",
    "X_all =df_full .drop (columns =['Buying activity'])\n",
    "y_all =df_full ['Buying activity']\n",
    "# We get probabilities from the model (1 - \"decreased\")\n",
    "proba =best_model .predict_proba (X_all )[:,1 ]\n",
    "# We save in Datafrem\n",
    "df_segmentation =pd .DataFrame ({\n",
    "'id':X_all ['id'],\n",
    "'The probability of_':proba \n",
    "# 2. We attach profit\n",
    "df_segmentation =df_segmentation .merge (df_profit ,on ='id',how ='left')\n",
    "# 3. Add the necessary signs from df_full\n",
    "df_segmentation =df_segmentation .merge (\n",
    "df_full [['id','Promotion_no -buying','Popular_Acategoria']],\n",
    "on ='id',\n",
    "how ='left'\n",
    "# Let's see the result\n",
    "display (df_segmentation .head ())\n"
    "# Segment A: high profit and high risk of outflow\n",
    "segment_A =df_segmentation [\n",
    "(df_segmentation ['The probability of_']>0.7 )&\n",
    "(df_segmentation ['Profit']>4.0 )\n",
    "# Segment B: high share of promotional purchases and average profits\n",
    "segment_B =df_segmentation [\n",
    "(df_segmentation ['Promotion_no -buying']>0.7 )&\n",
    "(df_segmentation ['Profit'].between (2.0 ,4.0 ))\n",
    "# C: buyers of goods for children with risk of outflow\n",
    "segment_C =df_segmentation [\n",
    "(df_segmentation ['Popular_Acategoria']=='Goods for children')&\n",
    "(df_segmentation ['The probability of_']>0.5 )\n",
    "# Let's check the dimensions\n",
    "print (f'Segment a: {segment_a .shape [0]} clients')\n",
    "print (f'Segment b: {segment_b .shape [0]} clients')\n",
    "print (f'C: {segment_c .shape [0]} clients')\n"
    "### 12.7.4 Conclusion by step 8: Customer segmentation and recommendations\n",
    "Based on the predicted probabilities of outflow and business signs, three key segments were allocated:\n",
    "- ** segment A ** - 185 clients with high profit and at the same time high risk of outflow.\n",
    "  Critically important for retention: priority No. 1.\n",
    "- ** segment b ** - 80 clients with a high share of promotional purchases and average profit.  \n",
    "  Demand rethinking marketing approaches: unprofitability risk.\n",
    "- ** segment C ** - 132 clients buying goods for children, with an increased risk of outflow.  \n",
    "  Possible problems in holding the target category: It is important to check the user path and commodity supply.\n",
    "The segments do not intersect and reflect three different business bolt.  \n",
    "The result of the segmentation was the construction of a focus for subsequent actions: retention, correction of marketing and analysis of problematic product areas."
    "# 1. Visualization of distributions\n",
    "plt .figure (figsize =(12 ,8 ))\n",
    "plt .subplot (2 ,2 ,1 )\n",
    "plt .hist (segment_A ['Profit'],bins =10 ,edgecolor ='black')\n",
    "plt .title ('Profit (segment A)')\n",
    "plt .subplot (2 ,2 ,2 )\n",
    "plt .hist (segment_A ['The probability of_'],bins =10 ,edgecolor ='black')\n",
    "plt .title ('The probability of outflow (segment A)')\n",
    "plt .subplot (2 ,2 ,3 )\n",
    "plt .hist (segment_A ['Promotion_no -buying'],bins =10 ,edgecolor ='black')\n",
    "plt .title ('Share of promotional purchases (segment A)')\n",
    "plt .subplot (2 ,2 ,4 )\n",
    "segment_A ['Popular_Acategoria'].value_counts ().plot (kind ='bar')\n",
    "plt .title ('Popular categories (segment A)')\n",
    "plt .xticks (rotation =45 )\n",
    "plt .tight_layout ()\n",
    "plt .show ()\n",
    "# 2. Consolidated characteristics\n",
    "print (\"Average profit:\",round (segment_A ['Profit'].mean (),2 ))\n",
    "print (\"The average probability of outflow:\",round (segment_A ['The probability of_'].mean (),2 ))\n",
    "print (\"The average share of promotional purchases:\",round (segment_A ['Promotion_no -buying'].mean (),2 ))\n",
    "print (\"\\ nkategoria of goods:\")\n",
    "print (segment_A ['Popular_Acategoria'].value_counts ())\n"
    "print (df_full .columns )\n"
    "# 1. Add to each segment data on revenue from df_full\n",
    "cols_to_add =['id','Popular_Acategoria','Revenue_ -reinforcing','Revenue_ predecessor','Revenue_ -flowing']\n",
    "\n",
    "segment_A_ext =segment_A .merge (df_full [cols_to_add ],on =['id','Popular_Acategoria'],how ='left')\n",
    "segment_B_ext =segment_B .merge (df_full [cols_to_add ],on =['id','Popular_Acategoria'],how ='left')\n",
    "segment_C_ext =segment_C .merge (df_full [cols_to_add ],on =['id','Popular_Acategoria'],how ='left')\n",
    "\n",
    "# 2. We calculate the delta\n",
    "for df in [segment_A_ext ,segment_B_ext ,segment_C_ext ]:\n",
    "    df ['Text']=df ['Revenue_ -flowing']-df ['Revenue_ predecessor']\n",
    "    df ['pre -_o']=df ['Revenue_ predecessor']-df ['Revenue_ -reinforcing']\n",
    "\n",
    "    # 3. Function: conclusion and visualization\n",
    "def revenue_dynamics_summary_with_plot (df_segment ,name ):\n",
    "    summary =df_segment .groupby ('Popular_Acategoria')[['Text','pre -_o']].mean ().round (2 )\n",
    "    print (f\"\\ n {name}: average dynamics of revenue by categories\")\n",
    "    display (summary )\n",
    "\n",
    "    # Visualization\n",
    "    ax =summary .plot (kind ='bar',figsize =(10 ,5 ),edgecolor ='black')\n",
    "    plt .title (f'The average dynamics of revenue by categories - {name}')\n",
    "    plt .xlabel ('Category')\n",
    "    plt .ylabel ('The average change in revenue')\n",
    "    plt .axhline (0 ,color ='gray',linestyle ='--')\n",
    "    plt .xticks (rotation =45 )\n",
    "    plt .grid (axis ='y')\n",
    "    plt .tight_layout ()\n",
    "    plt .show ()\n",
    "\n",
    "    # 4. Apply to each segment\n",
    "revenue_dynamics_summary_with_plot (segment_A_ext ,\"Segment a\")\n",
    "revenue_dynamics_summary_with_plot (segment_B_ext ,\"Segment b\")\n",
    "revenue_dynamics_summary_with_plot (segment_C_ext ,\"Segment C.\")\n"
    "### conclusion by step 8: Customer segmentation and recommendations\n",
    "\n",
    "** Selected segment: **  \n",
    "The analysis was carried out for ** segment A ** - customers with high profit (> 4.0) and a high probability of outflow (> 0.7). This segment includes ** 186 users **, which is more than 12% of the entire base and gives the greatest contribution to the revenue.\n",
    "\n",
    "** Dynamics of revenue: **\n",
    "- The largest revenue growth in the last month is observed in the categories ** ‚ÄúCosmetics and accessories‚Äù **, ** ‚Äúkitchen dishes‚Äù ** and ** ‚Äúhome textiles‚Äù **.\n",
    "- The outflow of revenue in the penultimate month was marked in ** ‚Äútechnique for beauty‚Äù ** and ** ‚Äúproducts for children‚Äù **, which signals a possible reduction in interest in these categories.\n",
    "- Despite the growth in the current month, ** ‚ÄúProducts for children‚Äù ** show weak stability - the revenue fluctuates on them.\n",
    "\n",
    "** The proposed measures: **\n",
    "- Configure personalized newsletters with content in your favorite categories.\n",
    "- divide customers into subgroups in sensitivity to shares:\n",
    "  - one - to offer benefits without shares,\n",
    "  - To others - use discounts, bonuses, cashback.\n",
    "- Add privileges for the most profitable customers: priority support, managerial line, early access to new products.\n",
    "\n",
    "** Business renewal: **\n",
    "- The loss of a client from this segment can lead to ** loss of 4-7 units of profit **.\n",
    "- Holding even ** 10% of customers ** from the segment can significantly increase ** LTV ** and reduce ** CAC **.\n",
    "- Clients of the segment have pronounced preferences, which facilitates targeting and personalization.\n",
    "\n",
    "**Conclusion:**  \n",
    "The segment A requires priority attention: due to high profits, significant outflow and potential to retain. A personalized approach and optimization of proposals will save customers and increase their value for business."
    "### General conclusion on the project\n",
    "#### Task\n",
    "The online store ‚ÄúIn one click‚Äù set the task-to predict the likelihood of ** reducing the purchasing activity of regular customers ** to determine ** risk groups ** and offer ** personalized retention measures **.\n",
    "#### Data and Sources\n",
    "Four tables were provided:\n",
    "- `market_file.csv` - marketing, behavioral and categorical features.\n",
    "- `market_money.csv` - monthly revenue.\n",
    "- `market_time.csv` - the time spent on the site.\n",
    "- `money.csv` - the final profit from the client.\n",
    "#### data exposure\n",
    "- fixed typos in categorical features and names of periods.\n",
    "- Duplicates and gaps are checked and cleaned.\n",
    "- 3 clients with zero revenue in one of the months are excluded.\n",
    "- One extreme ejection on revenue is filtered.\n",
    "- The tables are combined by `ID`, the final dataset contains ** 1297 lines ** and ** 18 signs **.\n",
    "#### Research Analysis (EDA)\n",
    "- Clients with reduced activity more often:\n",
    "  - less often agree to mailings,\n",
    "  - spend less time on the site,\n",
    "  - do less views and purchases,\n",
    "  - stronger depends on shares,\n",
    "  - They bring less revenue.\n",
    "- Correlations confirm: involvement (time, views, pages) reduces the risk of outflow.\n",
    "- Signs with the largest number of emissions: marketing activity and promotional purchases.\n",
    "- Signs are left as potentially informative for the model.\n",
    "### Model Training\n",
    "- 4 models are trained (`logisticregression`,` decisiontree`, `kneighbors`,` svc`) with two scaling methods (`standardscaler`,` minmaxscaler`).\n",
    "- Used by `RandomizedSearchcv`, metric - ** roc auc **.\n",
    "- ** The best model: ** `svc` with` minmaxscler`, roc auc = ** 0.9019 **.\n",
    "- High results also showed:\n",
    "  - `LogisticregResion` - ROC AUC = ** 0.9012 **\n",
    "  - `kneighborsclassifier` - ROC AUC = ** 0.8876 **\n",
    "#### interpretation of the model\n",
    "- For interpretation, `LogisticregResion` (transparent model) was used.\n",
    "- Shap indicators revealed that the most impact on the outflow has:\n",
    "  - `Pages_za_vizit`,` average_ -examination_kategoria_za_vizit` - indicators of involvement.\n",
    "  - `unpaid_products_Tuk_Cartal` and` promotional_ -purchases` - abandoned baskets and sensitivity to discounts.\n",
    "  - `Market_activ_6_Mes` and old revenues - traces of fatigue from communications.\n",
    "#### Customer segmentation\n",
    "Based on the probability of outflow, profit and behavioral signs, ** three segments **:\n",
    "- ** segment A ** - 185 clients: high profit (> 4.0) and high risk of outflow (> 0.7).\n",
    "  - * average profit: * 4.76  \n",
    "  - * The average probability of outflow: * 0.95  \n",
    "  - * Popular categories: * \"Products for children\", \"Cosmetics\", \"Home textile\"  \n",
    "  - * negative dynamics: * in the category \"Technique for Beauty\"\n",
    "- ** segment b ** - 80 clients: high share of promotional purchases (> 0.7), average profit (2‚Äì4).  \n",
    "  - Possible risk of discount dependence.  \n",
    "  - The fall in revenue in a number of categories has been noticed, especially \"goods for children\"\n",
    "- ** segment C ** - 132 clients: customers of the category \"Products forChildren ‚Äùwith an increased risk of outflow (> 0.5).  \n",
    "  - unstable revenue, problems in the category: outdated assortment, ux, seasonality are possible\n",
    "#### Dynamics of revenue\n",
    "- In the segment a:\n",
    "  - the greatest decline in \"Technique for Beauty\" (the previous month is a sharp decrease).\n",
    "- In the segment b:\n",
    "  - Negative dynamics in ‚ÄúProducts for children‚Äù and partially in ‚ÄúTechnique‚Äù.\n",
    "- In the segment C:\n",
    "  - ‚ÄúProducts for children‚Äù - weak stability, negative dynamics in the penultimate month.\n",
    "#### business recommendations\n",
    "** segment A ** - Priority retention:\n",
    "- personalize marketing in your favorite categories.\n",
    "- Add bonus programs, priority support, exclusive offers.\n",
    "- Do not use discounts - holding the price of service and privileges.\n",
    "** segment b ** - control of discount dependence:\n",
    "- Conduct A/B tests to reduce the number of shares.\n",
    "- Introduce alternative mechanics: cashback, cumulative points, privileges for loyalty.\n",
    "** segment C ** - Analysis of the product category:\n",
    "- Conduct UX assessment and study of usability on the site.\n",
    "- To double -check the assortment and seasonality in ‚ÄúProducts for children‚Äù.\n",
    "- Collect feedback - find out what is the reason for a decrease in interest."
    "### comment on the recommendations:\n",
    "Segment A - priority retention\n",
    "Why is it highlighted:\n",
    "These are customers with high profit and high probability of outflow. Their loss will be the most painful for business, so the retention of this group is the main priority.\n",
    "Recommendations:\n",
    "Personalized proposals:\n",
    "Supporting interest in purchases through targeted campaigns in your favorite categories (‚Äúgoods for children‚Äù, ‚Äúcosmetics‚Äù, etc.) - to increase involvement without reducing prices.\n",
    "Bonus programs / Priority service:\n",
    "Since the segment is loyal and brings profit, it is logical to strengthen emotional binding through intangible advantages - for example, priority in delivery, access to new collections, and the allocated manager.\n",
    "Do not use discounts:\n",
    "These clients are already buying at the ‚Äúfull price‚Äù, and are not prone to promotional offers - discounts will be ineffective and reduce the margin.\n",
    "Segment b - control of discount dependence\n",
    "Why is it highlighted:\n",
    "These clients make a lot of purchases for shares, while bringing not the highest profit. There is a risk that they are used to discounts and do not buy without them.\n",
    "Recommendations:\n",
    "A/b test to restrict discounts:\n",
    "Check how such customers will react to the lack of shares. Perhaps they will continue to buy without discounts, and then it will be possible to improve margin.\n",
    "Transition to other benefits:\n",
    "Instead of constant discounts - bonuses, accumulative systems, or suggestions on recommendations. This reduces the dependence on the promo and teaches the client to another type of value.\n",
    "Segment C-commodity and ux analysis\n",
    "Why is it highlighted:\n",
    "These clients have the main interest - the category of \"goods for children.\" At the same time, they demonstrate a high risk of outflow. The analysis showed the negative dynamics of revenue in this category.\n",
    "Recommendations:\n",
    "Checking a product assortment:\n",
    "There is a chance that the product is out of place, it does not correspond to expectations, or it got worse. This requires a review.\n",
    "Feedback collection:\n",
    "It is recommended to conduct a small study among this group to find out the causes of care.\n",
    "Analysis UX and windows:\n",
    "Perhaps problems are not in goods, but in their view: filters, description, photo, positioning. This is also worth checking before investments in the purchase."
